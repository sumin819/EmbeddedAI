{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
    "from ptflops import get_model_complexity_info\n",
    "import time\n",
    "from torch.fx import symbolic_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device 설정\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 설정\n",
    "data_dir = \"data\"\n",
    "# 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['with_mask', 'without_mask']\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "print(f\"Classes: {dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train:Val:Test = 70:15:15 분할\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskClassifier, self).__init__()\n",
    "        \n",
    "        # Feature Extraction - 더 얕은 구조로 변경\n",
    "        self.features = nn.Sequential(\n",
    "            # First Block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Second Block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Third Block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "        )\n",
    "        \n",
    "        # Classifier - 더 단순한 구조로 변경\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning(model, amount=0.5):\n",
    "    # Initialize pruner\n",
    "    example_inputs = torch.randn(1, 3, 224, 224)\n",
    "    imp = tp.importance.MagnitudeImportance()\n",
    "    \n",
    "    pruner = tp.pruner.MagnitudePruner(\n",
    "        model,\n",
    "        example_inputs,\n",
    "        importance=imp,\n",
    "        ch_sparsity=amount,\n",
    "        ignored_layers=[model.classifier[2]]  # Ignore final classification layer\n",
    "    )\n",
    "    \n",
    "    # Create pruning plan\n",
    "    pruner.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function과 optimizer 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    best_val_acc = 0.0\n",
    "    optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.0001\n",
    "    )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Progress Bar에 현재 배치의 accuracy 표시\n",
    "            batch_acc = 100. * correct / total\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{batch_acc:.2f}%'\n",
    "            })\n",
    "    \n",
    "    # 최종 학습 결과 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    final_acc = 100. * correct / total\n",
    "    print(\"\\n=== Final Training Results ===\")\n",
    "    print(f\"Final Validation Accuracy: {final_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 함수\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")  # Progress Bar 추가\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix(batch_accuracy=(preds == labels).float().mean().item())\n",
    "            \n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size and other metrics in detail\"\"\"\n",
    "    # 파라미터 수 계산\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # 레이어별 파라미터 수 계산\n",
    "    layer_params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_params[name] = param.numel()\n",
    "    \n",
    "    # 메모리 사용량 계산\n",
    "    param_size = 0\n",
    "    buffer_size = 0\n",
    "    \n",
    "    # 파라미터 메모리 계산\n",
    "    for param in model.state_dict().values():\n",
    "        param_size += param.numel() * param.element_size()\n",
    "    \n",
    "    # 버퍼 메모리 계산 (BatchNorm 등의 running stats)\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.numel() * buffer.element_size()\n",
    "    \n",
    "    total_size = param_size + buffer_size\n",
    "    \n",
    "    # 모델 구조 분석\n",
    "    layer_types = {}\n",
    "    for name, module in model.named_modules():\n",
    "        layer_type = module.__class__.__name__\n",
    "        if layer_type not in layer_types:\n",
    "            layer_types[layer_type] = 1\n",
    "        else:\n",
    "            layer_types[layer_type] += 1\n",
    "    \n",
    "    return {\n",
    "        'model_stats': {\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'total_parameters': total_params,\n",
    "            'param_memory_bytes': param_size,\n",
    "            'buffer_memory_bytes': buffer_size,\n",
    "            'total_memory_bytes': total_size,\n",
    "            'total_memory_mb': total_size / (1024 * 1024)\n",
    "        },\n",
    "        'layer_parameters': layer_params,\n",
    "        'layer_types': layer_types\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_pruned_model(pruning_ratio, train_loader, val_loader, num_epochs=20):\n",
    "    \"\"\"Create, prune, and train a model with specified pruning ratio\"\"\"\n",
    "    # 새 모델 생성 및 기존 가중치 로드\n",
    "    model = MaskClassifier().to(device)\n",
    "    model.load_state_dict(torch.load(\"mask_classifier.pth\"))\n",
    "    \n",
    "    # Pruning 적용\n",
    "    print(f\"\\nApplying {pruning_ratio*100}% pruning...\")\n",
    "    apply_pruning(model, amount=pruning_ratio)\n",
    "    \n",
    "    # 모델 학습\n",
    "    print(f\"\\nTraining {pruning_ratio*100}% pruned model...\")\n",
    "    train_model(model, train_loader, val_loader, num_epochs)\n",
    "    \n",
    "    # 모델 저장\n",
    "    save_path = f\"pruned_model_{int(pruning_ratio*100)}percent.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Saved pruned model to {save_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compare_model_sizes(models_dict):\n",
    "    \"\"\"Compare and display sizes of multiple models\"\"\"\n",
    "    print(\"\\n=== Model Size Comparison ===\")\n",
    "    print(\"\\nModel Statistics:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Model':20} {'Trainable Params':>15} {'Total Params':>15} {'Memory (MB)':>15}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        analysis = get_model_size(model)\n",
    "        stats = analysis['model_stats']\n",
    "        print(f\"{name:20} {stats['trainable_parameters']:15,d} {stats['total_parameters']:15,d} {stats['total_memory_mb']:15.2f}\")\n",
    "    \n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying 30.0% pruning...\n",
      "\n",
      "Training 30.0% pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 144/144 [00:57<00:00,  2.51batch/s, loss=0.0872, acc=92.06%]\n",
      "Epoch 2/10: 100%|██████████| 144/144 [00:58<00:00,  2.45batch/s, loss=0.0244, acc=92.37%]\n",
      "Epoch 3/10: 100%|██████████| 144/144 [01:04<00:00,  2.24batch/s, loss=0.4220, acc=92.28%]\n",
      "Epoch 4/10: 100%|██████████| 144/144 [01:03<00:00,  2.26batch/s, loss=0.0761, acc=92.76%]\n",
      "Epoch 5/10: 100%|██████████| 144/144 [00:58<00:00,  2.46batch/s, loss=0.9113, acc=92.28%]\n",
      "Epoch 6/10: 100%|██████████| 144/144 [00:59<00:00,  2.40batch/s, loss=1.3649, acc=92.46%]\n",
      "Epoch 7/10: 100%|██████████| 144/144 [01:03<00:00,  2.26batch/s, loss=0.0306, acc=93.02%]\n",
      "Epoch 8/10: 100%|██████████| 144/144 [00:59<00:00,  2.43batch/s, loss=0.0627, acc=93.02%]\n",
      "Epoch 9/10: 100%|██████████| 144/144 [01:02<00:00,  2.29batch/s, loss=0.6857, acc=93.50%]\n",
      "Epoch 10/10: 100%|██████████| 144/144 [00:58<00:00,  2.44batch/s, loss=0.0340, acc=92.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Training Results ===\n",
      "Final Validation Accuracy: 95.93%\n",
      "Saved pruned model to pruned_model_30percent.pth\n",
      "\n",
      "Applying 50.0% pruning...\n",
      "\n",
      "Training 50.0% pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 144/144 [00:41<00:00,  3.48batch/s, loss=0.0932, acc=89.14%]\n",
      "Epoch 2/10: 100%|██████████| 144/144 [00:38<00:00,  3.72batch/s, loss=0.2846, acc=91.93%]\n",
      "Epoch 3/10: 100%|██████████| 144/144 [00:39<00:00,  3.68batch/s, loss=0.0750, acc=91.67%]\n",
      "Epoch 4/10: 100%|██████████| 144/144 [00:39<00:00,  3.61batch/s, loss=0.5212, acc=91.02%]\n",
      "Epoch 5/10: 100%|██████████| 144/144 [00:40<00:00,  3.53batch/s, loss=0.1637, acc=91.10%]\n",
      "Epoch 6/10: 100%|██████████| 144/144 [00:37<00:00,  3.80batch/s, loss=0.0179, acc=92.28%]\n",
      "Epoch 7/10: 100%|██████████| 144/144 [00:39<00:00,  3.69batch/s, loss=0.0912, acc=91.89%]\n",
      "Epoch 8/10: 100%|██████████| 144/144 [00:39<00:00,  3.63batch/s, loss=0.2185, acc=92.46%]\n",
      "Epoch 9/10: 100%|██████████| 144/144 [00:38<00:00,  3.77batch/s, loss=0.2811, acc=92.46%]\n",
      "Epoch 10/10: 100%|██████████| 144/144 [00:41<00:00,  3.45batch/s, loss=0.1014, acc=92.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Training Results ===\n",
      "Final Validation Accuracy: 95.93%\n",
      "Saved pruned model to pruned_model_50percent.pth\n",
      "\n",
      "Applying 70.0% pruning...\n",
      "\n",
      "Training 70.0% pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 144/144 [00:33<00:00,  4.36batch/s, loss=1.1030, acc=86.57%]\n",
      "Epoch 2/10: 100%|██████████| 144/144 [00:29<00:00,  4.83batch/s, loss=0.5936, acc=88.84%]\n",
      "Epoch 3/10: 100%|██████████| 144/144 [00:29<00:00,  4.84batch/s, loss=0.3117, acc=90.54%]\n",
      "Epoch 4/10: 100%|██████████| 144/144 [00:30<00:00,  4.69batch/s, loss=0.1768, acc=91.41%]\n",
      "Epoch 5/10: 100%|██████████| 144/144 [00:31<00:00,  4.55batch/s, loss=0.2462, acc=91.23%]\n",
      "Epoch 6/10: 100%|██████████| 144/144 [00:37<00:00,  3.88batch/s, loss=1.0470, acc=90.49%]\n",
      "Epoch 7/10: 100%|██████████| 144/144 [00:32<00:00,  4.41batch/s, loss=0.0722, acc=91.28%]\n",
      "Epoch 8/10: 100%|██████████| 144/144 [00:31<00:00,  4.53batch/s, loss=0.0353, acc=91.06%]\n",
      "Epoch 9/10: 100%|██████████| 144/144 [00:31<00:00,  4.50batch/s, loss=0.3209, acc=90.45%]\n",
      "Epoch 10/10: 100%|██████████| 144/144 [00:32<00:00,  4.40batch/s, loss=0.0726, acc=90.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Training Results ===\n",
      "Final Validation Accuracy: 94.70%\n",
      "Saved pruned model to pruned_model_70percent.pth\n",
      "\n",
      "=== Model Size Comparison ===\n",
      "\n",
      "Model Statistics:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model                Trainable Params    Total Params     Memory (MB)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pruned_30%                    45,195          45,195            0.17\n",
      "Pruned_50%                    23,938          23,938            0.09\n",
      "Pruned_70%                     8,556           8,556            0.03\n",
      "Original                      93,954          93,954            0.36\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 실행 코드\n",
    "pruning_ratios = [0.3, 0.5, 0.7]\n",
    "pruned_models = {}\n",
    "\n",
    "# 각 프루닝 비율에 대해 모델 생성 및 학습\n",
    "for ratio in pruning_ratios:\n",
    "    pruned_models[f\"Pruned_{int(ratio*100)}%\"] = create_and_train_pruned_model(\n",
    "        ratio, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        num_epochs=10\n",
    "    )\n",
    "\n",
    "# 원본 모델 추가\n",
    "original_model = MaskClassifier().to(device)\n",
    "original_model.load_state_dict(torch.load(\"mask_classifier.pth\"))\n",
    "pruned_models[\"Original\"] = original_model\n",
    "\n",
    "# 모델 크기 비교\n",
    "compare_model_sizes(pruned_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
