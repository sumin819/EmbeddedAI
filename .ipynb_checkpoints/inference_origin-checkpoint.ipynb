{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from torchinfo import torchinfo\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskClassifier, self).__init__()\n",
    "        \n",
    "        # Feature Extraction - 더 얕은 구조로 변경\n",
    "        self.features = nn.Sequential(\n",
    "            # First Block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Second Block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Third Block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "        )\n",
    "        \n",
    "        # Classifier - 더 단순한 구조로 변경\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 모델 로드\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "model = MaskClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskClassifier(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout2d(p=0.2, inplace=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout2d(p=0.2, inplace=False)\n",
       "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Dropout2d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 가중치 로드\n",
    "model.load_state_dict(torch.load(\"mask_classifier.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 함수\n",
    "def preprocess_image(image):\n",
    "    # 이미지 크기 조정\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    \n",
    "    # BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 정규화 (ImageNet stats)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # (H, W, C) -> (C, H, W)\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    \n",
    "    # numpy -> tensor\n",
    "    image = torch.FloatTensor(image).unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_webcam():\n",
    "    # 모델 로드\n",
    "    model = MaskClassifier().to(device)\n",
    "    model.load_state_dict(torch.load(\"mask_classifier.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # 얼굴 검출을 위한 cascade classifier 로드\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    # cascade_path = \"/usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml\"  # 일반적인 Linux 설치 경로\n",
    "    # face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    dataset_classes = [\"With Mask\", \"Without Mask\"]\n",
    "\n",
    "    print(\"Press 'q' to quit.\")\n",
    "    \n",
    "    # 성능 측정을 위한 변수들\n",
    "    frame_count = 0\n",
    "    total_time = 0\n",
    "    total_inference_time = 0\n",
    "    prev_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    while True:\n",
    "        frame_start_time = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 좌우 반전\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # 밝기 조정 (화면 어둡게 하기)\n",
    "        brightness_offset = 50  # 밝기를 낮출 값 (0~255)\n",
    "        frame = cv2.convertScaleAbs(frame, alpha=1, beta=-brightness_offset)\n",
    "        \n",
    "        # 얼굴 검출\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(60, 60)\n",
    "        )\n",
    "        \n",
    "        # 얼굴 검출\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(60, 60)\n",
    "        )\n",
    "        \n",
    "        # FPS 계산\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "        \n",
    "        # 검출된 얼굴에 대해 마스크 분류 수행\n",
    "        for (x, y, w, h) in faces:\n",
    "            # 얼굴 영역 추출\n",
    "            face_roi = frame[max(0, y-30):min(frame.shape[0], y+h+30), \n",
    "                           max(0, x-30):min(frame.shape[1], x+w+30)]\n",
    "            \n",
    "            if face_roi.size != 0:\n",
    "                # 추론 시간 측정 시작\n",
    "                inference_start = time.time()\n",
    "                \n",
    "                # 마스크 분류\n",
    "                input_tensor = preprocess_image(face_roi).to(device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                \n",
    "                # 추론 시간 측정 종료\n",
    "                inference_time = time.time() - inference_start\n",
    "                total_inference_time += inference_time\n",
    "                \n",
    "                # 결과 표시\n",
    "                label = dataset_classes[pred.item()]\n",
    "                color = (0, 255, 0) if \"With\" in label else (0, 0, 255)\n",
    "                \n",
    "                # 얼굴 영역 표시\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                \n",
    "                # 라벨과 추론 시간 표시\n",
    "                cv2.putText(frame, f\"{label} ({inference_time*1000:.1f}ms)\", \n",
    "                          (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "        \n",
    "        # 프레임 처리 완료 시간 계산\n",
    "        frame_time = time.time() - frame_start_time\n",
    "        total_time += frame_time\n",
    "        frame_count += 1\n",
    "        \n",
    "        # 성능 지표 표시\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"Frame Time: {frame_time*1000:.1f}ms\", (10, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # 결과 표시\n",
    "        cv2.imshow(\"Mask Detection\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 최종 성능 통계 계산\n",
    "    avg_fps = frame_count / total_time\n",
    "    avg_frame_time = total_time / frame_count\n",
    "    avg_inference_time = total_inference_time / frame_count if frame_count > 0 else 0\n",
    "    \n",
    "    print(\"\\n=== Performance Statistics ===\")\n",
    "    print(f\"Total Frames: {frame_count}\")\n",
    "    print(f\"Average FPS: {avg_fps:.1f}\")\n",
    "    print(f\"Average Frame Time: {avg_frame_time*1000:.1f}ms\")\n",
    "    print(f\"Average Inference Time: {avg_inference_time*1000:.1f}ms\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n",
      "\n",
      "=== Performance Statistics ===\n",
      "Total Frames: 145\n",
      "Average FPS: 8.1\n",
      "Average Frame Time: 122.9ms\n",
      "Average Inference Time: 26.3ms\n"
     ]
    }
   ],
   "source": [
    "# 실시간 웹캠 추론 실행\n",
    "infer_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer_csi_camera():\n",
    "    \"\"\"\n",
    "    Jetson Nano의 CSI 카메라를 활용한 실시간 추론 함수\n",
    "    \"\"\"\n",
    "    # 모델 로드\n",
    "    model = MaskClassifier().to(device)\n",
    "    model.load_state_dict(torch.load(\"mask_classifier.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # 얼굴 검출을 위한 cascade classifier 로드\n",
    "    # face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    cascade_path = \"/usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml\"  # 일반적인 Linux 설치 경로\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    \n",
    "    # GStreamer 파이프라인 정의\n",
    "    gst_pipeline = (\n",
    "        \"nvarguscamerasrc ! \"\n",
    "        \"video/x-raw(memory:NVMM), width=640, height=480, format=(string)NV12, framerate=30/1 ! \"\n",
    "        \"nvvidconv flip-method=0 ! \"\n",
    "        \"video/x-raw, width=640, height=480, format=(string)BGRx ! \"\n",
    "        \"videoconvert ! \"\n",
    "        \"video/x-raw, format=(string)BGR ! appsink\"\n",
    "    )\n",
    "\n",
    "    cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)\n",
    "    if not cap.isOpened():\n",
    "        print(\"CSI 카메라를 열 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    dataset_classes = [\"With Mask\", \"Without Mask\"]\n",
    "    print(\"Press 'q' to quit.\")\n",
    "    \n",
    "    # 성능 측정을 위한 변수들\n",
    "    frame_count = 0\n",
    "    total_time = 0\n",
    "    total_inference_time = 0\n",
    "    prev_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    while True:\n",
    "        frame_start_time = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"카메라 프레임을 읽을 수 없습니다.\")\n",
    "            break\n",
    "        \n",
    "        # 좌우 반전\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # 밝기 조정 (화면 어둡게 하기)\n",
    "        brightness_offset = 50  # 밝기를 낮출 값 (0~255)\n",
    "        frame = cv2.convertScaleAbs(frame, alpha=1, beta=-brightness_offset)\n",
    "        \n",
    "        # 얼굴 검출\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(60, 60)\n",
    "        )\n",
    "        \n",
    "        # 얼굴 검출\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(60, 60)\n",
    "        )\n",
    "        \n",
    "        # FPS 계산\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "        \n",
    "        # 검출된 얼굴에 대해 마스크 분류 수행\n",
    "        for (x, y, w, h) in faces:\n",
    "            # 얼굴 영역 추출\n",
    "            face_roi = frame[max(0, y-30):min(frame.shape[0], y+h+30), \n",
    "                           max(0, x-30):min(frame.shape[1], x+w+30)]\n",
    "            \n",
    "            if face_roi.size != 0:\n",
    "                # 추론 시간 측정 시작\n",
    "                inference_start = time.time()\n",
    "                \n",
    "                # 마스크 분류\n",
    "                input_tensor = preprocess_image(face_roi).to(device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                \n",
    "                # 추론 시간 측정 종료\n",
    "                inference_time = time.time() - inference_start\n",
    "                total_inference_time += inference_time\n",
    "                \n",
    "                # 결과 표시\n",
    "                label = dataset_classes[pred.item()]\n",
    "                color = (0, 255, 0) if \"With\" in label else (0, 0, 255)\n",
    "                \n",
    "                # 얼굴 영역 표시\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                \n",
    "                # 라벨과 추론 시간 표시\n",
    "                cv2.putText(frame, f\"{label} ({inference_time*1000:.1f}ms)\", \n",
    "                          (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "        \n",
    "        # 프레임 처리 완료 시간 계산\n",
    "        frame_time = time.time() - frame_start_time\n",
    "        total_time += frame_time\n",
    "        frame_count += 1\n",
    "        \n",
    "        # 성능 지표 표시\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"Frame Time: {frame_time*1000:.1f}ms\", (10, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # 결과 표시\n",
    "        cv2.imshow(\"CSI Camera Mask Detection\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 최종 성능 통계 계산\n",
    "    avg_fps = frame_count / total_time\n",
    "    avg_frame_time = total_time / frame_count\n",
    "    avg_inference_time = total_inference_time / frame_count if frame_count > 0 else 0\n",
    "    \n",
    "    print(\"\\n=== Performance Statistics ===\")\n",
    "    print(f\"Total Frames: {frame_count}\")\n",
    "    print(f\"Average FPS: {avg_fps:.1f}\")\n",
    "    print(f\"Average Frame Time: {avg_frame_time*1000:.1f}ms\")\n",
    "    print(f\"Average Inference Time: {avg_inference_time*1000:.1f}ms\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSI 카메라를 열 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# 5. 실시간 추론 실행\n",
    "infer_csi_camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Calculate total, trainable, zero and non-zero parameters\"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    zero_params = 0\n",
    "    nonzero_params = 0\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        total_params += p.numel()\n",
    "        if p.requires_grad:\n",
    "            trainable_params += p.numel()\n",
    "        \n",
    "        # Count zero and non-zero parameters\n",
    "        zero_params += torch.sum(p == 0).item()\n",
    "        nonzero_params += torch.sum(p != 0).item()\n",
    "    \n",
    "    return {\n",
    "        'total': total_params,\n",
    "        'trainable': trainable_params,\n",
    "        'zero': zero_params,\n",
    "        'nonzero': nonzero_params\n",
    "    }\n",
    "\n",
    "def calculate_flops(model):\n",
    "    \"\"\"Calculate FLOPs considering zero parameters\"\"\"\n",
    "    input_tensor = torch.randn(1, 3, 128, 128)\n",
    "    macs, params = profile(model, inputs=(input_tensor,))\n",
    "    \n",
    "    flops = 2 * macs  # Convert MACs to FLOPs\n",
    "    \n",
    "    # 0이 아닌 파라미터의 비율 계산\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:  # weight 파라미터만 고려\n",
    "            total_params += param.numel()\n",
    "            nonzero_params += torch.sum(param != 0).item()\n",
    "    \n",
    "    # 실제 수행되는 연산량 추정\n",
    "    sparsity = 1 - (nonzero_params / total_params)\n",
    "    actual_flops = flops * (1 - sparsity)\n",
    "    \n",
    "    return {\n",
    "        'raw_flops': flops,\n",
    "        'actual_flops': actual_flops,\n",
    "        'sparsity': sparsity * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_analysis():\n",
    "    params = count_parameters(model)\n",
    "    flops_info = calculate_flops(model)\n",
    "    \n",
    "    print(\"\\n=== Model Analysis ===\")\n",
    "    print(f\"Total Parameters: {params['total']:,}\")\n",
    "    print(f\"├─ Non-zero Parameters: {params['nonzero']:,} ({params['nonzero']/params['total']*100:.1f}%)\")\n",
    "    print(f\"└─ Zero Parameters: {params['zero']:,} ({params['zero']/params['total']*100:.1f}%)\")\n",
    "    print(f\"Model Sparsity: {params['zero']/params['total']*100:.1f}%\")\n",
    "    print(f\"\\nFLOPs Analysis:\")\n",
    "    print(f\"├─ Raw FLOPs: {flops_info['raw_flops']:,}\")\n",
    "    print(f\"├─ Weight Sparsity: {flops_info['sparsity']:.1f}%\")\n",
    "    print(f\"└─ Estimated Actual FLOPs: {flops_info['actual_flops']:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\n",
      "=== Model Analysis ===\n",
      "Total Parameters: 93,954\n",
      "├─ Non-zero Parameters: 93,954 (100.0%)\n",
      "└─ Zero Parameters: 0 (0.0%)\n",
      "Model Sparsity: 0.0%\n",
      "\n",
      "FLOPs Analysis:\n",
      "├─ Raw FLOPs: 337,707,776.0\n",
      "├─ Weight Sparsity: 0.0%\n",
      "└─ Estimated Actual FLOPs: 337,707,776\n"
     ]
    }
   ],
   "source": [
    "print_model_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Architecture Summary ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "=====================================================================================================================================================================\n",
       "MaskClassifier                           --                        --                        --                        --                        --\n",
       "├─Sequential: 1-1                        [1, 3, 128, 128]          [1, 128, 16, 16]          --                        --                        --\n",
       "│    └─Conv2d: 2-1                       [1, 3, 128, 128]          [1, 32, 128, 128]         896                       [3, 32, 3, 3]             14,680,064\n",
       "│    └─BatchNorm2d: 2-2                  [1, 32, 128, 128]         [1, 32, 128, 128]         64                        [32]                      64\n",
       "│    └─ReLU: 2-3                         [1, 32, 128, 128]         [1, 32, 128, 128]         --                        --                        --\n",
       "│    └─MaxPool2d: 2-4                    [1, 32, 128, 128]         [1, 32, 64, 64]           --                        --                        --\n",
       "│    └─Dropout2d: 2-5                    [1, 32, 64, 64]           [1, 32, 64, 64]           --                        --                        --\n",
       "│    └─Conv2d: 2-6                       [1, 32, 64, 64]           [1, 64, 64, 64]           18,496                    [32, 64, 3, 3]            75,759,616\n",
       "│    └─BatchNorm2d: 2-7                  [1, 64, 64, 64]           [1, 64, 64, 64]           128                       [64]                      128\n",
       "│    └─ReLU: 2-8                         [1, 64, 64, 64]           [1, 64, 64, 64]           --                        --                        --\n",
       "│    └─MaxPool2d: 2-9                    [1, 64, 64, 64]           [1, 64, 32, 32]           --                        --                        --\n",
       "│    └─Dropout2d: 2-10                   [1, 64, 32, 32]           [1, 64, 32, 32]           --                        --                        --\n",
       "│    └─Conv2d: 2-11                      [1, 64, 32, 32]           [1, 128, 32, 32]          73,856                    [64, 128, 3, 3]           75,628,544\n",
       "│    └─BatchNorm2d: 2-12                 [1, 128, 32, 32]          [1, 128, 32, 32]          256                       [128]                     256\n",
       "│    └─ReLU: 2-13                        [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --                        --\n",
       "│    └─MaxPool2d: 2-14                   [1, 128, 32, 32]          [1, 128, 16, 16]          --                        --                        --\n",
       "│    └─Dropout2d: 2-15                   [1, 128, 16, 16]          [1, 128, 16, 16]          --                        --                        --\n",
       "├─Sequential: 1-2                        [1, 128, 16, 16]          [1, 2]                    --                        --                        --\n",
       "│    └─AdaptiveAvgPool2d: 2-16           [1, 128, 16, 16]          [1, 128, 1, 1]            --                        --                        --\n",
       "│    └─Flatten: 2-17                     [1, 128, 1, 1]            [1, 128]                  --                        --                        --\n",
       "│    └─Linear: 2-18                      [1, 128]                  [1, 2]                    258                       [128, 2]                  258\n",
       "=====================================================================================================================================================================\n",
       "Total params: 93,954\n",
       "Trainable params: 93,954\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 166.07\n",
       "=====================================================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 14.68\n",
       "Params size (MB): 0.38\n",
       "Estimated Total Size (MB): 15.25\n",
       "====================================================================================================================================================================="
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detailed model summary\n",
    "print(\"\\n=== Model Architecture Summary ===\")\n",
    "torchinfo.summary(model, input_size=(1, 3, 128, 128), \n",
    "                 col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
