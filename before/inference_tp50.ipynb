{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from torchinfo import torchinfo\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskClassifier, self).__init__()\n",
    "        \n",
    "        # Feature Extraction - 프루닝된 구조\n",
    "        self.features = nn.Sequential(\n",
    "            # First Block\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Second Block\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Third Block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 모델 로드\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "model = MaskClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskClassifier(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout2d(p=0.2, inplace=False)\n",
       "    (5): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout2d(p=0.2, inplace=False)\n",
       "    (10): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Dropout2d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 가중치 로드\n",
    "model.load_state_dict(torch.load(\"pt_mask_classifier.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 함수\n",
    "def preprocess_image(image):\n",
    "    # 이미지 크기 조정\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    \n",
    "    # BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 정규화 (ImageNet stats)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # (H, W, C) -> (C, H, W)\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    \n",
    "    # numpy -> tensor\n",
    "    image = torch.FloatTensor(image).unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_webcam():\n",
    "    # 모델 로드\n",
    "    model = MaskClassifier().to(device)\n",
    "    model.load_state_dict(torch.load(\"pt_mask_classifier.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # 얼굴 검출을 위한 cascade classifier 로드\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    # cascade_path = \"/usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml\"  # 일반적인 Linux 설치 경로\n",
    "    # face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    dataset_classes = [\"With Mask\", \"Without Mask\"]\n",
    "\n",
    "    print(\"Press 'q' to quit.\")\n",
    "    \n",
    "    # 성능 측정을 위한 변수들\n",
    "    frame_count = 0\n",
    "    total_time = 0\n",
    "    total_inference_time = 0\n",
    "    prev_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    while True:\n",
    "        frame_start_time = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 좌우 반전\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # 밝기 조정 (화면 어둡게 하기)\n",
    "        brightness_offset = 0  # 밝기를 낮출 값 (0~255)\n",
    "        frame = cv2.convertScaleAbs(frame, alpha=1, beta=-brightness_offset)\n",
    "        \n",
    "        # 얼굴 검출\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(60, 60)\n",
    "        )\n",
    "        \n",
    "        # FPS 계산\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "        \n",
    "        # 검출된 얼굴에 대해 마스크 분류 수행\n",
    "        for (x, y, w, h) in faces:\n",
    "            # 얼굴 영역 추출\n",
    "            face_roi = frame[max(0, y-30):min(frame.shape[0], y+h+30), \n",
    "                           max(0, x-30):min(frame.shape[1], x+w+30)]\n",
    "            \n",
    "            if face_roi.size != 0:\n",
    "                # 추론 시간 측정 시작\n",
    "                inference_start = time.time()\n",
    "                \n",
    "                # 마스크 분류\n",
    "                input_tensor = preprocess_image(face_roi).to(device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                \n",
    "                # 추론 시간 측정 종료\n",
    "                inference_time = time.time() - inference_start\n",
    "                total_inference_time += inference_time\n",
    "                \n",
    "                # 결과 표시\n",
    "                label = dataset_classes[pred.item()]\n",
    "                color = (0, 255, 0) if \"With\" in label else (0, 0, 255)\n",
    "                \n",
    "                # 얼굴 영역 표시\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                \n",
    "                # 라벨과 추론 시간 표시\n",
    "                cv2.putText(frame, f\"{label} ({inference_time*1000:.1f}ms)\", \n",
    "                          (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "        \n",
    "        # 프레임 처리 완료 시간 계산\n",
    "        frame_time = time.time() - frame_start_time\n",
    "        total_time += frame_time\n",
    "        frame_count += 1\n",
    "        \n",
    "        # 성능 지표 표시\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"Frame Time: {frame_time*1000:.1f}ms\", (10, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # 결과 표시\n",
    "        cv2.imshow(\"Mask Detection\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 최종 성능 통계 계산\n",
    "    avg_fps = frame_count / total_time\n",
    "    avg_frame_time = total_time / frame_count\n",
    "    avg_inference_time = total_inference_time / frame_count if frame_count > 0 else 0\n",
    "    \n",
    "    print(\"\\n=== Performance Statistics ===\")\n",
    "    print(f\"Total Frames: {frame_count}\")\n",
    "    print(f\"Average FPS: {avg_fps:.1f}\")\n",
    "    print(f\"Average Frame Time: {avg_frame_time*1000:.1f}ms\")\n",
    "    print(f\"Average Inference Time: {avg_inference_time*1000:.1f}ms\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n",
      "\n",
      "=== Performance Statistics ===\n",
      "Total Frames: 162\n",
      "Average FPS: 17.7\n",
      "Average Frame Time: 56.6ms\n",
      "Average Inference Time: 12.2ms\n"
     ]
    }
   ],
   "source": [
    "# 실시간 웹캠 추론 실행\n",
    "infer_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer_csi_camera():\n",
    "    \"\"\"\n",
    "    Jetson Nano의 CSI 카메라를 활용한 실시간 추론 함수\n",
    "    \"\"\"\n",
    "    # 모델 로드\n",
    "    model = MaskClassifier().to(device)\n",
    "    model.load_state_dict(torch.load(\"pt_mask_classifier.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # 얼굴 검출을 위한 cascade classifier 로드\n",
    "    # face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    cascade_path = \"/usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml\"  # 일반적인 Linux 설치 경로\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    \n",
    "    # GStreamer 파이프라인 정의\n",
    "    gst_pipeline = (\n",
    "        \"nvarguscamerasrc ! \"\n",
    "        \"video/x-raw(memory:NVMM), width=640, height=480, format=(string)NV12, framerate=30/1 ! \"\n",
    "        \"nvvidconv flip-method=0 ! \"\n",
    "        \"video/x-raw, width=640, height=480, format=(string)BGRx ! \"\n",
    "        \"videoconvert ! \"\n",
    "        \"video/x-raw, format=(string)BGR ! appsink\"\n",
    "    )\n",
    "\n",
    "    cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)\n",
    "    if not cap.isOpened():\n",
    "        print(\"CSI 카메라를 열 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    dataset_classes = [\"With Mask\", \"Without Mask\"]\n",
    "    print(\"Press 'q' to quit.\")\n",
    "    \n",
    "    # 성능 측정을 위한 변수들\n",
    "    frame_count = 0\n",
    "    total_time = 0\n",
    "    total_inference_time = 0\n",
    "    prev_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    while True:\n",
    "        frame_start_time = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"카메라 프레임을 읽을 수 없습니다.\")\n",
    "            break\n",
    "        \n",
    "        # 좌우 반전\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # 밝기 조정 (화면 어둡게 하기)\n",
    "        brightness_offset = 50  # 밝기를 낮출 값 (0~255)\n",
    "        frame = cv2.convertScaleAbs(frame, alpha=1, beta=-brightness_offset)\n",
    "        \n",
    "        # 얼굴 검출\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(60, 60)\n",
    "        )\n",
    "        \n",
    "        # FPS 계산\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "        \n",
    "        # 검출된 얼굴에 대해 마스크 분류 수행\n",
    "        for (x, y, w, h) in faces:\n",
    "            # 얼굴 영역 추출\n",
    "            face_roi = frame[max(0, y-30):min(frame.shape[0], y+h+30), \n",
    "                           max(0, x-30):min(frame.shape[1], x+w+30)]\n",
    "            \n",
    "            if face_roi.size != 0:\n",
    "                # 추론 시간 측정 시작\n",
    "                inference_start = time.time()\n",
    "                \n",
    "                # 마스크 분류\n",
    "                input_tensor = preprocess_image(face_roi).to(device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                \n",
    "                # 추론 시간 측정 종료\n",
    "                inference_time = time.time() - inference_start\n",
    "                total_inference_time += inference_time\n",
    "                \n",
    "                # 결과 표시\n",
    "                label = dataset_classes[pred.item()]\n",
    "                color = (0, 255, 0) if \"With\" in label else (0, 0, 255)\n",
    "                \n",
    "                # 얼굴 영역 표시\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                \n",
    "                # 라벨과 추론 시간 표시\n",
    "                cv2.putText(frame, f\"{label} ({inference_time*1000:.1f}ms)\", \n",
    "                          (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "        \n",
    "        # 프레임 처리 완료 시간 계산\n",
    "        frame_time = time.time() - frame_start_time\n",
    "        total_time += frame_time\n",
    "        frame_count += 1\n",
    "        \n",
    "        # 성능 지표 표시\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"Frame Time: {frame_time*1000:.1f}ms\", (10, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # 결과 표시\n",
    "        cv2.imshow(\"CSI Camera Mask Detection\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 최종 성능 통계 계산\n",
    "    avg_fps = frame_count / total_time\n",
    "    avg_frame_time = total_time / frame_count\n",
    "    avg_inference_time = total_inference_time / frame_count if frame_count > 0 else 0\n",
    "    \n",
    "    print(\"\\n=== Performance Statistics ===\")\n",
    "    print(f\"Total Frames: {frame_count}\")\n",
    "    print(f\"Average FPS: {avg_fps:.1f}\")\n",
    "    print(f\"Average Frame Time: {avg_frame_time*1000:.1f}ms\")\n",
    "    print(f\"Average Inference Time: {avg_inference_time*1000:.1f}ms\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSI 카메라를 열 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# 5. 실시간 추론 실행\n",
    "infer_csi_camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Calculate total, trainable, zero and non-zero parameters\"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    zero_params = 0\n",
    "    nonzero_params = 0\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        total_params += p.numel()\n",
    "        if p.requires_grad:\n",
    "            trainable_params += p.numel()\n",
    "        \n",
    "        # Count zero and non-zero parameters\n",
    "        zero_params += torch.sum(p == 0).item()\n",
    "        nonzero_params += torch.sum(p != 0).item()\n",
    "    \n",
    "    return {\n",
    "        'total': total_params,\n",
    "        'trainable': trainable_params,\n",
    "        'zero': zero_params,\n",
    "        'nonzero': nonzero_params\n",
    "    }\n",
    "\n",
    "def calculate_flops(model):\n",
    "    \"\"\"Calculate FLOPs considering zero parameters\"\"\"\n",
    "    input_tensor = torch.randn(1, 3, 128, 128)\n",
    "    macs, params = profile(model, inputs=(input_tensor,))\n",
    "    \n",
    "    flops = 2 * macs  # Convert MACs to FLOPs\n",
    "    \n",
    "    # 0이 아닌 파라미터의 비율 계산\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:  # weight 파라미터만 고려\n",
    "            total_params += param.numel()\n",
    "            nonzero_params += torch.sum(param != 0).item()\n",
    "    \n",
    "    # 실제 수행되는 연산량 추정\n",
    "    sparsity = 1 - (nonzero_params / total_params)\n",
    "    actual_flops = flops * (1 - sparsity)\n",
    "    \n",
    "    return {\n",
    "        'raw_flops': flops,\n",
    "        'actual_flops': actual_flops,\n",
    "        'sparsity': sparsity * 100\n",
    "    }\n",
    "    \n",
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in various units\"\"\"\n",
    "    # state_dict()를 통한 실제 메모리 사용량 계산\n",
    "    param_size = 0\n",
    "    buffer_size = 0\n",
    "    \n",
    "    # 파라미터 크기 계산\n",
    "    for param in model.state_dict().values():\n",
    "        param_size += param.numel() * param.element_size()\n",
    "    \n",
    "    # 버퍼 크기 계산 (BN의 running mean/var 등)\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.numel() * buffer.element_size()\n",
    "        \n",
    "    # 총 크기 계산\n",
    "    total_size = param_size + buffer_size\n",
    "    \n",
    "    # 다양한 단위로 변환\n",
    "    size_bytes = total_size\n",
    "    size_kb = total_size / 1024\n",
    "    size_mb = size_kb / 1024\n",
    "    \n",
    "    return {\n",
    "        'bytes': size_bytes,\n",
    "        'kb': size_kb,\n",
    "        'mb': size_mb,\n",
    "        'params': sum(p.numel() for p in model.parameters()),\n",
    "        'param_size': param_size,\n",
    "        'buffer_size': buffer_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_analysis():\n",
    "    \"\"\"모델 분석 결과 출력 함수\"\"\"\n",
    "    # 기존 분석\n",
    "    params = count_parameters(model)\n",
    "    flops_info = calculate_flops(model)\n",
    "    \n",
    "    # 모델 크기 분석 추가\n",
    "    size_info = get_model_size(model)\n",
    "    \n",
    "    print(\"\\n=== Model Analysis ===\")\n",
    "    print(f\"Total Parameters: {params['total']:,}\")\n",
    "    print(f\"├─ Non-zero Parameters: {params['nonzero']:,} ({params['nonzero']/params['total']*100:.1f}%)\")\n",
    "    print(f\"└─ Zero Parameters: {params['zero']:,} ({params['zero']/params['total']*100:.1f}%)\")\n",
    "    print(f\"Model Sparsity: {params['zero']/params['total']*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nModel Size:\")\n",
    "    print(f\"├─ Total Size: {size_info['mb']:.2f} MB\")\n",
    "    print(f\"├─ Parameter Memory: {size_info['param_size']/1024/1024:.2f} MB\")\n",
    "    print(f\"└─ Buffer Memory: {size_info['buffer_size']/1024/1024:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nFLOPs Analysis:\")\n",
    "    print(f\"├─ Raw FLOPs: {flops_info['raw_flops']:,}\")\n",
    "    print(f\"├─ Weight Sparsity: {flops_info['sparsity']:.1f}%\")\n",
    "    print(f\"└─ Estimated Actual FLOPs: {flops_info['actual_flops']:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\n",
      "=== Model Analysis ===\n",
      "Total Parameters: 23,938\n",
      "├─ Non-zero Parameters: 23,938 (100.0%)\n",
      "└─ Zero Parameters: 0 (0.0%)\n",
      "Model Sparsity: 0.0%\n",
      "\n",
      "Model Size:\n",
      "├─ Total Size: 0.09 MB\n",
      "├─ Parameter Memory: 0.09 MB\n",
      "└─ Buffer Memory: 0.00 MB\n",
      "\n",
      "FLOPs Analysis:\n",
      "├─ Raw FLOPs: 93,356,416.0\n",
      "├─ Weight Sparsity: 0.0%\n",
      "└─ Estimated Actual FLOPs: 93,356,416\n"
     ]
    }
   ],
   "source": [
    "print_model_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Architecture Summary ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "=====================================================================================================================================================================\n",
       "MaskClassifier                           --                        --                        --                        --                        --\n",
       "├─Sequential: 1-1                        [1, 3, 128, 128]          [1, 64, 16, 16]           --                        --                        --\n",
       "│    └─Conv2d: 2-1                       [1, 3, 128, 128]          [1, 16, 128, 128]         448                       [3, 16, 3, 3]             7,340,032\n",
       "│    └─BatchNorm2d: 2-2                  [1, 16, 128, 128]         [1, 16, 128, 128]         32                        [16]                      32\n",
       "│    └─ReLU: 2-3                         [1, 16, 128, 128]         [1, 16, 128, 128]         --                        --                        --\n",
       "│    └─MaxPool2d: 2-4                    [1, 16, 128, 128]         [1, 16, 64, 64]           --                        --                        --\n",
       "│    └─Dropout2d: 2-5                    [1, 16, 64, 64]           [1, 16, 64, 64]           --                        --                        --\n",
       "│    └─Conv2d: 2-6                       [1, 16, 64, 64]           [1, 32, 64, 64]           4,640                     [16, 32, 3, 3]            19,005,440\n",
       "│    └─BatchNorm2d: 2-7                  [1, 32, 64, 64]           [1, 32, 64, 64]           64                        [32]                      64\n",
       "│    └─ReLU: 2-8                         [1, 32, 64, 64]           [1, 32, 64, 64]           --                        --                        --\n",
       "│    └─MaxPool2d: 2-9                    [1, 32, 64, 64]           [1, 32, 32, 32]           --                        --                        --\n",
       "│    └─Dropout2d: 2-10                   [1, 32, 32, 32]           [1, 32, 32, 32]           --                        --                        --\n",
       "│    └─Conv2d: 2-11                      [1, 32, 32, 32]           [1, 64, 32, 32]           18,496                    [32, 64, 3, 3]            18,939,904\n",
       "│    └─BatchNorm2d: 2-12                 [1, 64, 32, 32]           [1, 64, 32, 32]           128                       [64]                      128\n",
       "│    └─ReLU: 2-13                        [1, 64, 32, 32]           [1, 64, 32, 32]           --                        --                        --\n",
       "│    └─MaxPool2d: 2-14                   [1, 64, 32, 32]           [1, 64, 16, 16]           --                        --                        --\n",
       "│    └─Dropout2d: 2-15                   [1, 64, 16, 16]           [1, 64, 16, 16]           --                        --                        --\n",
       "├─Sequential: 1-2                        [1, 64, 16, 16]           [1, 2]                    --                        --                        --\n",
       "│    └─AdaptiveAvgPool2d: 2-16           [1, 64, 16, 16]           [1, 64, 1, 1]             --                        --                        --\n",
       "│    └─Flatten: 2-17                     [1, 64, 1, 1]             [1, 64]                   --                        --                        --\n",
       "│    └─Linear: 2-18                      [1, 64]                   [1, 2]                    130                       [64, 2]                   130\n",
       "=====================================================================================================================================================================\n",
       "Total params: 23,938\n",
       "Trainable params: 23,938\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 45.29\n",
       "=====================================================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 7.34\n",
       "Params size (MB): 0.10\n",
       "Estimated Total Size (MB): 7.63\n",
       "====================================================================================================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detailed model summary\n",
    "print(\"\\n=== Model Architecture Summary ===\")\n",
    "torchinfo.summary(model, input_size=(1, 3, 128, 128), \n",
    "                 col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
