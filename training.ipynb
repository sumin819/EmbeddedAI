{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import cv2\n",
    "# from evaluator import ModelEvaluator\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
    "from ptflops import get_model_complexity_info\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model, device=None):\n",
    "        \"\"\"\n",
    "        모델 평가 클래스 초기화\n",
    "        Args:\n",
    "            model (torch.nn.Module): 평가할 PyTorch 모델\n",
    "            device (torch.device, optional): 사용할 디바이스 (CPU/GPU)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def evaluate_model_size(self):\n",
    "        \"\"\"\n",
    "        모델의 총 파라미터 수와 학습 가능한 파라미터 수를 출력\n",
    "        \"\"\"\n",
    "        params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Total Parameters: {params:,}\")\n",
    "        print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "        return params, trainable_params\n",
    "\n",
    "    def evaluate_flops(self, input_size=(3, 128, 112)):\n",
    "        \"\"\"\n",
    "        모델의 FLOPs(Floating Point Operations) 계산\n",
    "        Args:\n",
    "            input_size (tuple): 입력 텐서 크기 (채널, 높이, 너비)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            macs, params = get_model_complexity_info(self.model, input_size, as_strings=False, verbose=False)\n",
    "            flops = macs * 2  # FLOPs = MACs * 2\n",
    "            print(f\"FLOPs: {flops / 1e6:.2f} MFLOPs\")\n",
    "        return flops\n",
    "\n",
    "\n",
    "    def evaluate_inference_speed(self, input_size=(3, 128, 128), iterations=100):\n",
    "        \"\"\"\n",
    "        모델의 평균 추론 속도 측정\n",
    "        Args:\n",
    "            input_size (tuple): 입력 텐서 크기 (채널, 높이, 너비)\n",
    "            iterations (int): 추론 반복 횟수\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        input_tensor = torch.randn(1, *input_size).to(self.device)\n",
    "        torch.cuda.synchronize()  # GPU 사용 시 동기화\n",
    "\n",
    "        start_time = time.time()\n",
    "        for _ in range(iterations):\n",
    "            with torch.no_grad():\n",
    "                self.model(input_tensor)\n",
    "        torch.cuda.synchronize()  # GPU 사용 시 동기화\n",
    "        end_time = time.time()\n",
    "\n",
    "        avg_time_per_inference = (end_time - start_time) / iterations\n",
    "        print(f\"Average Inference Time: {avg_time_per_inference * 1000:.2f} ms\")\n",
    "        return avg_time_per_inference\n",
    "\n",
    "    def summarize_model(self, input_size=(3, 128, 128)):\n",
    "        \"\"\"\n",
    "        모델의 레이어별 출력 크기와 파라미터 요약\n",
    "        Args:\n",
    "            input_size (tuple): 입력 텐서 크기 (채널, 높이, 너비)\n",
    "        \"\"\"\n",
    "        print(\"\\n===== Model Summary =====\")\n",
    "        summary(self.model, input_size=input_size)\n",
    "\n",
    "    def evaluate_all(self, input_size=(3, 128, 128), iterations=100):\n",
    "        \"\"\"\n",
    "        모델의 모든 평가(파라미터 수, FLOPs, 실행 속도, 요약)를 수행\n",
    "        Args:\n",
    "            input_size (tuple): 입력 텐서 크기 (채널, 높이, 너비)\n",
    "            iterations (int): 추론 반복 횟수\n",
    "        \"\"\"\n",
    "        print(\"\\n===== Model Evaluation =====\")\n",
    "        self.evaluate_model_size()\n",
    "        self.evaluate_flops(input_size=input_size)\n",
    "        # self.evaluate_inference_speed(input_size=input_size, iterations=iterations)\n",
    "        self.summarize_model(input_size=input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로\n",
    "data_dir = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['with_mask', 'without_mask']\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "# dataset = ImageFolder(root=data_dir)\n",
    "\n",
    "# 클래스 정보 출력\n",
    "print(f\"Classes: {dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train:Val:Test = 70:15:15 분할\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskClassifier, self).__init__()\n",
    "        \n",
    "        # Feature Extraction - 더 얕은 구조로 변경\n",
    "        self.features = nn.Sequential(\n",
    "            # First Block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Second Block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Third Block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "        )\n",
    "        \n",
    "        # Classifier - 더 단순한 구조로 변경\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Loss function 수정 - 클래스 불균형을 고려한 가중치 추가\n",
    "# weights = torch.tensor([1.0, 1.0]).to(device)  # 필요시 클래스별 가중치 조정\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Learning rate와 optimizer 수정\n",
    "learning_rate = 0.001  # 더 작은 learning rate 사용\n",
    "num_epoch = 50  # epoch 수 감소\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.0001  # 더 작은 weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Progress Bar에 현재 배치의 accuracy 표시\n",
    "            batch_acc = 100. * correct / total\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{batch_acc:.2f}%'\n",
    "            })\n",
    "    \n",
    "    # 최종 학습 결과 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    final_acc = 100. * correct / total\n",
    "    print(\"\\n=== Final Training Results ===\")\n",
    "    print(f\"Final Validation Accuracy: {final_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 72/72 [00:40<00:00,  1.76batch/s, loss=0.1321, acc=84.51%]\n",
      "Epoch 2/50: 100%|██████████| 72/72 [00:40<00:00,  1.79batch/s, loss=0.0597, acc=87.32%]\n",
      "Epoch 3/50: 100%|██████████| 72/72 [00:40<00:00,  1.79batch/s, loss=0.5383, acc=88.02%]\n",
      "Epoch 4/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.3716, acc=88.55%]\n",
      "Epoch 5/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=1.2232, acc=88.72%]\n",
      "Epoch 6/50: 100%|██████████| 72/72 [00:39<00:00,  1.81batch/s, loss=0.1054, acc=88.24%]\n",
      "Epoch 7/50: 100%|██████████| 72/72 [00:39<00:00,  1.81batch/s, loss=0.3859, acc=89.03%]\n",
      "Epoch 8/50: 100%|██████████| 72/72 [00:39<00:00,  1.81batch/s, loss=0.2126, acc=90.04%]\n",
      "Epoch 9/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.3036, acc=89.73%]\n",
      "Epoch 10/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.2473, acc=90.39%]\n",
      "Epoch 11/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.4693, acc=90.52%]\n",
      "Epoch 12/50: 100%|██████████| 72/72 [00:40<00:00,  1.76batch/s, loss=0.3636, acc=90.52%]\n",
      "Epoch 13/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.6179, acc=89.95%]\n",
      "Epoch 14/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.1941, acc=90.13%]\n",
      "Epoch 15/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.0348, acc=90.30%]\n",
      "Epoch 16/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=1.5280, acc=90.61%]\n",
      "Epoch 17/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.0966, acc=90.57%]\n",
      "Epoch 18/50: 100%|██████████| 72/72 [00:39<00:00,  1.81batch/s, loss=0.6085, acc=91.36%]\n",
      "Epoch 19/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.4145, acc=91.18%]\n",
      "Epoch 20/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.6500, acc=91.44%]\n",
      "Epoch 21/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.5194, acc=91.84%]\n",
      "Epoch 22/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.1483, acc=92.80%]\n",
      "Epoch 23/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.1292, acc=91.93%]\n",
      "Epoch 24/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.2020, acc=91.49%]\n",
      "Epoch 25/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.3238, acc=91.97%]\n",
      "Epoch 26/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.8936, acc=92.23%]\n",
      "Epoch 27/50: 100%|██████████| 72/72 [00:39<00:00,  1.81batch/s, loss=0.1698, acc=93.02%]\n",
      "Epoch 28/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.4784, acc=92.58%]\n",
      "Epoch 29/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.3048, acc=92.80%]\n",
      "Epoch 30/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.5563, acc=92.54%]\n",
      "Epoch 31/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.1733, acc=93.02%]\n",
      "Epoch 32/50: 100%|██████████| 72/72 [00:39<00:00,  1.84batch/s, loss=0.3865, acc=92.89%]\n",
      "Epoch 33/50: 100%|██████████| 72/72 [00:39<00:00,  1.81batch/s, loss=0.0654, acc=92.10%]\n",
      "Epoch 34/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.1017, acc=93.29%]\n",
      "Epoch 35/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.3624, acc=92.89%]\n",
      "Epoch 36/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.0565, acc=92.50%]\n",
      "Epoch 37/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.3293, acc=93.37%]\n",
      "Epoch 38/50: 100%|██████████| 72/72 [00:39<00:00,  1.81batch/s, loss=0.2592, acc=92.63%]\n",
      "Epoch 39/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.1045, acc=93.42%]\n",
      "Epoch 40/50: 100%|██████████| 72/72 [00:39<00:00,  1.82batch/s, loss=0.0321, acc=94.08%]\n",
      "Epoch 41/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.4834, acc=94.16%]\n",
      "Epoch 42/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.6946, acc=93.68%]\n",
      "Epoch 43/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.2641, acc=93.59%]\n",
      "Epoch 44/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.5550, acc=93.90%]\n",
      "Epoch 45/50: 100%|██████████| 72/72 [00:39<00:00,  1.81batch/s, loss=0.1169, acc=94.30%]\n",
      "Epoch 46/50: 100%|██████████| 72/72 [00:39<00:00,  1.81batch/s, loss=0.0534, acc=94.30%]\n",
      "Epoch 47/50: 100%|██████████| 72/72 [00:39<00:00,  1.84batch/s, loss=0.0832, acc=93.42%]\n",
      "Epoch 48/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.0437, acc=93.90%]\n",
      "Epoch 49/50: 100%|██████████| 72/72 [00:39<00:00,  1.83batch/s, loss=0.3492, acc=94.16%]\n",
      "Epoch 50/50: 100%|██████████| 72/72 [00:39<00:00,  1.84batch/s, loss=0.0219, acc=93.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Training Results ===\n",
      "Final Validation Accuracy: 94.67%\n"
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "train_model(model, train_loader, val_loader, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 함수\n",
    "def test_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (torch.nn.Module): 평가할 모델\n",
    "        test_loader (DataLoader): 테스트 데이터 로더\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")  # Progress Bar 추가\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Progress Bar 상태 업데이트 (현재 배치의 예측 결과 일부 표시)\n",
    "            progress_bar.set_postfix(batch_accuracy=(preds == labels).float().mean().item())\n",
    "\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 16/16 [00:03<00:00,  4.79batch/s, batch_accuracy=1]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   with_mask       1.00      0.92      0.96       246\n",
      "without_mask       0.92      1.00      0.96       243\n",
      "\n",
      "    accuracy                           0.96       489\n",
      "   macro avg       0.96      0.96      0.96       489\n",
      "weighted avg       0.96      0.96      0.96       489\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 실행\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹캠 테스트 함수\n",
    "def webcam_test(model):\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(0)  # 웹캠 열기\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    print(\"Press 'q' to quit.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, pred = torch.max(output, 1)\n",
    "\n",
    "        label = dataset.classes[pred.item()]\n",
    "        cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# 웹캠 테스트 실행\n",
    "webcam_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 코드\n",
    "def save_model(model, path=\"mask_classifier.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to mask_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# 학습 후 모델 저장 및 평가\n",
    "save_model(model, \"mask_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Evaluation =====\n",
      "Total Parameters: 93,954\n",
      "Trainable Parameters: 93,954\n",
      "MaskClassifier(\n",
      "  93.95 k, 100.000% Params, 169.77 MMac, 98.912% MACs, \n",
      "  (features): Sequential(\n",
      "    93.7 k, 99.725% Params, 169.74 MMac, 98.893% MACs, \n",
      "    (0): Conv2d(896, 0.954% Params, 14.68 MMac, 8.553% MACs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, 0.068% Params, 1.05 MMac, 0.611% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(0, 0.000% Params, 524.29 KMac, 0.305% MACs, inplace=True)\n",
      "    (3): MaxPool2d(0, 0.000% Params, 524.29 KMac, 0.305% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.2, inplace=False)\n",
      "    (5): Conv2d(18.5 k, 19.686% Params, 75.76 MMac, 44.139% MACs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): BatchNorm2d(128, 0.136% Params, 524.29 KMac, 0.305% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(0, 0.000% Params, 262.14 KMac, 0.153% MACs, inplace=True)\n",
      "    (8): MaxPool2d(0, 0.000% Params, 262.14 KMac, 0.153% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.2, inplace=False)\n",
      "    (10): Conv2d(73.86 k, 78.609% Params, 75.63 MMac, 44.063% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, 0.272% Params, 262.14 KMac, 0.153% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(0, 0.000% Params, 131.07 KMac, 0.076% MACs, inplace=True)\n",
      "    (13): MaxPool2d(0, 0.000% Params, 131.07 KMac, 0.076% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Dropout2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.2, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    258, 0.275% Params, 33.03 KMac, 0.019% MACs, \n",
      "    (0): AdaptiveAvgPool2d(0, 0.000% Params, 32.77 KMac, 0.019% MACs, output_size=(1, 1))\n",
      "    (1): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "    (2): Linear(258, 0.275% Params, 258.0 Mac, 0.000% MACs, in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "FLOPs: 343.28 MFLOPs\n",
      "\n",
      "===== Model Summary =====\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             896\n",
      "       BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
      "              ReLU-3         [-1, 32, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 32, 64, 64]               0\n",
      "         Dropout2d-5           [-1, 32, 64, 64]               0\n",
      "            Conv2d-6           [-1, 64, 64, 64]          18,496\n",
      "       BatchNorm2d-7           [-1, 64, 64, 64]             128\n",
      "              ReLU-8           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-9           [-1, 64, 32, 32]               0\n",
      "        Dropout2d-10           [-1, 64, 32, 32]               0\n",
      "           Conv2d-11          [-1, 128, 32, 32]          73,856\n",
      "      BatchNorm2d-12          [-1, 128, 32, 32]             256\n",
      "             ReLU-13          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-14          [-1, 128, 16, 16]               0\n",
      "        Dropout2d-15          [-1, 128, 16, 16]               0\n",
      "AdaptiveAvgPool2d-16            [-1, 128, 1, 1]               0\n",
      "          Flatten-17                  [-1, 128]               0\n",
      "           Linear-18                    [-1, 2]             258\n",
      "================================================================\n",
      "Total params: 93,954\n",
      "Trainable params: 93,954\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 24.50\n",
      "Params size (MB): 0.36\n",
      "Estimated Total Size (MB): 25.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의 및 학습 완료 후\n",
    "evaluator = ModelEvaluator(model, device=device)\n",
    "evaluator.evaluate_all(input_size=(3, 128, 128), iterations=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
